{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEh4q3/MTX3/boscNlkR/y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alabassy/ASS-11/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bORDiCThg1FG",
        "outputId": "7b17fd82-5eeb-4260-8bb3-8ed01836cc7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Forward Result ---\n",
            "Predicted Probabilities:\n",
            "  الأهلي: 0.3955\n",
            "  نادي: 0.1771\n",
            "  القرن: 0.2764\n",
            "  الحقيقي: 0.1511\n",
            "\n",
            "The model thinks the next word is: 'الأهلي'\n",
            "\n",
            "Loss = 0.246210\n",
            "\n",
            "--- Gradients ---\n",
            "\n",
            "dWhy:\n",
            "Neuron 0: [0.14228713896338213, 0.06371442967254094, 0.09944645511559749, -0.3054480237515206]\n",
            "Neuron 1: [-0.056293316573224395, -0.02520745434878575, -0.039344179809167296, 0.12084495073117744]\n",
            "\n",
            "dWhh:\n",
            "Neuron 0: [0.24968235695936683, -0.09878227973000182]\n",
            "Neuron 1: [-0.18826219377896705, 0.07448251015783303]\n",
            "\n",
            "dWxh:\n",
            "Input 0: [0.4821774569538144, 0.9189480753532197]\n",
            "Input 1: [0.4821774569538144, 0.9189480753532197]\n",
            "Input 2: [0.4821774569538144, 0.9189480753532197]\n",
            "Input 3: [0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# 1. Prepare the data\n",
        "words = [\"الأهلي\", \"نادي\", \"القرن\", \"الحقيقي\"]\n",
        "word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "vocab_size = len(words)\n",
        "\n",
        "def to_one_hot(idx, size):\n",
        "    vec = [0] * size\n",
        "    vec[idx] = 1\n",
        "    return vec\n",
        "\n",
        "inputs = [to_one_hot(word_to_idx[word], vocab_size) for word in [\"الأهلي\", \"نادي\", \"القرن\"]]\n",
        "\n",
        "# Target (one-hot vector for \"الحقيقي\")\n",
        "target_idx = word_to_idx[\"الحقيقي\"]\n",
        "target = [0] * vocab_size\n",
        "target[target_idx] = 1\n",
        "\n",
        "# 2. Initialize weights manually\n",
        "hidden_size = 2\n",
        "\n",
        "Wxh = [\n",
        "    [0.5, -0.4],\n",
        "    [-0.3, 0.8],\n",
        "    [0.7, -0.5],\n",
        "    [-0.6, 0.2]\n",
        "]  # (input_size x hidden_size)\n",
        "\n",
        "Whh = [\n",
        "    [0.2, -0.1],\n",
        "    [0.5, 0.3]\n",
        "]  # (hidden_size x hidden_size)\n",
        "\n",
        "Why = [\n",
        "    [0.6, -0.2, 0.3, -0.5],\n",
        "    [-0.1, 0.7, 0.4, 0.5]\n",
        "]  # (hidden_size x output_size)\n",
        "\n",
        "# 3. Helper functions\n",
        "def mat_vec_mul(mat, vec):\n",
        "    return [sum(m * v for m, v in zip(row, vec)) for row in zip(*mat)]\n",
        "\n",
        "def softmax(x):\n",
        "    exps = [math.exp(i) for i in x]\n",
        "    sum_exps = sum(exps)\n",
        "    return [j / sum_exps for j in exps]\n",
        "\n",
        "def vec_mse(a, b):\n",
        "    return sum((x - y)**2 for x, y in zip(a, b)) / len(a)\n",
        "\n",
        "# 4. Forward pass\n",
        "h = [0.0] * hidden_size  # initial hidden state\n",
        "\n",
        "for x in inputs:\n",
        "    xh = mat_vec_mul(Wxh, x)\n",
        "    hh = mat_vec_mul(Whh, h)\n",
        "    h = [math.tanh(xh_i + hh_i) for xh_i, hh_i in zip(xh, hh)]\n",
        "\n",
        "# Output layer\n",
        "y_raw = mat_vec_mul(Why, h)\n",
        "y_pred = softmax(y_raw)\n",
        "\n",
        "# 5. Loss\n",
        "loss = vec_mse(y_pred, target)\n",
        "\n",
        "# 6. Gradients\n",
        "# Derivative of MSE Loss w.r.t output\n",
        "dL_dy = [(2 * (y_pred[i] - target[i])) / len(target) for i in range(len(target))]\n",
        "\n",
        "# Gradients for Why\n",
        "dWhy = []\n",
        "for i in range(len(h)):\n",
        "    dWhy_row = []\n",
        "    for d in dL_dy:\n",
        "        dWhy_row.append(h[i] * d)\n",
        "    dWhy.append(dWhy_row)\n",
        "\n",
        "# Gradients for Whh\n",
        "dWhh = []\n",
        "for i in range(hidden_size):\n",
        "    dWhh_row = []\n",
        "    for j in range(hidden_size):\n",
        "        dWhh_row.append(h[i] * (1 - h[i]**2) * h[j])\n",
        "    dWhh.append(dWhh_row)\n",
        "\n",
        "# Gradients for Wxh\n",
        "dWxh = []\n",
        "for i in range(vocab_size):\n",
        "    dWxh_row = []\n",
        "    for j in range(hidden_size):\n",
        "        x_sum = sum(input_vec[i] for input_vec in inputs)\n",
        "        dWxh_row.append(x_sum * (1 - h[j]**2))\n",
        "    dWxh.append(dWxh_row)\n",
        "\n",
        "# 7. Light and casual output\n",
        "\n",
        "print(\"\\n--- Forward Result ---\")\n",
        "print(f\"Predicted Probabilities:\")\n",
        "for idx, prob in enumerate(y_pred):\n",
        "    print(f\"  {idx_to_word[idx]}: {prob:.4f}\")\n",
        "\n",
        "predicted_idx = y_pred.index(max(y_pred))\n",
        "predicted_word = idx_to_word[predicted_idx]\n",
        "print(f\"\\nThe model thinks the next word is: '{predicted_word}'\")\n",
        "\n",
        "print(f\"\\nLoss = {loss:.6f}\")\n",
        "\n",
        "print(\"\\n--- Gradients ---\")\n",
        "\n",
        "print(\"\\ndWhy:\")\n",
        "for i, row in enumerate(dWhy):\n",
        "    print(f\"Neuron {i}: {row}\")\n",
        "\n",
        "print(\"\\ndWhh:\")\n",
        "for i, row in enumerate(dWhh):\n",
        "    print(f\"Neuron {i}: {row}\")\n",
        "\n",
        "print(\"\\ndWxh:\")\n",
        "for i, row in enumerate(dWxh):\n",
        "    print(f\"Input {i}: {row}\")\n"
      ]
    }
  ]
}