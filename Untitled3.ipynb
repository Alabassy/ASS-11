{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFKYckCv1H0hobm5mPQa8W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alabassy/ASS-11/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bORDiCThg1FG",
        "outputId": "a452e1b4-cb89-44bc-9614-f89cb7dd6471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/500], Loss: 0.0000\n",
            "Epoch [200/500], Loss: 0.0000\n",
            "Epoch [300/500], Loss: 0.0000\n",
            "Epoch [400/500], Loss: 0.0000\n",
            "Epoch [500/500], Loss: 0.0000\n",
            "\n",
            "الكلمة المتوقعة: الحقيقي\n"
          ]
        }
      ],
      "source": [
        "sentence = [\"الأهلي\", \"نادي\", \"القرن\", \"الحقيقي\"]\n",
        "\n",
        "word_to_idx = {word: idx for idx, word in enumerate(sentence)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "vocab_size = len(sentence)\n",
        "\n",
        "inputs = [word_to_idx[\"الأهلي\"], word_to_idx[\"نادي\"], word_to_idx[\"القرن\"]]\n",
        "\n",
        "target = word_to_idx[\"الحقيقي\"]\n",
        "\n",
        "def to_one_hot(idx, vocab_size):\n",
        "    vec = [0] * vocab_size\n",
        "    vec[idx] = 1\n",
        "    return vec\n",
        "\n",
        "input_vectors = [to_one_hot(i, vocab_size) for i in inputs]\n",
        "\n",
        "import random\n",
        "\n",
        "def random_matrix(rows, cols):\n",
        "    return [[random.uniform(-1, 1) for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "W1 = random_matrix(vocab_size, 10)\n",
        "b1 = [0] * 10\n",
        "\n",
        "W2 = random_matrix(10, vocab_size)\n",
        "b2 = [0] * vocab_size\n",
        "\n",
        "def dot(vec, mat):\n",
        "    result = []\n",
        "    for col in zip(*mat):\n",
        "        result.append(sum(v * w for v, w in zip(vec, col)))\n",
        "    return result\n",
        "\n",
        "def add_bias(vec, bias):\n",
        "    return [v + b for v, b in zip(vec, bias)]\n",
        "\n",
        "def relu(vec):\n",
        "    return [max(0, v) for v in vec]\n",
        "\n",
        "def relu_derivative(vec):\n",
        "    return [1 if v > 0 else 0 for v in vec]\n",
        "\n",
        "def mse(pred, target):\n",
        "    return sum((p - t) ** 2 for p, t in zip(pred, target)) / len(pred)\n",
        "\n",
        "def mse_derivative(pred, target):\n",
        "    return [(p - t) * 2 / len(pred) for p, t in zip(pred, target)]\n",
        "\n",
        "def transpose(mat):\n",
        "    return list(map(list, zip(*mat)))\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 500\n",
        "\n",
        "target_vector = to_one_hot(target, vocab_size)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    hidden_outputs = []\n",
        "    final_outputs = []\n",
        "    for input_vec in input_vectors:\n",
        "        hidden = relu(add_bias(dot(input_vec, W1), b1))\n",
        "        output = add_bias(dot(hidden, W2), b2)\n",
        "        hidden_outputs.append(hidden)\n",
        "        final_outputs.append(output)\n",
        "\n",
        "    output_mean = [sum(x)/len(x) for x in zip(*final_outputs)]\n",
        "\n",
        "    loss = mse(output_mean, target_vector)\n",
        "\n",
        "    dloss = mse_derivative(output_mean, target_vector)\n",
        "\n",
        "    grad_W2 = [[0] * vocab_size for _ in range(10)]\n",
        "    grad_b2 = [0] * vocab_size\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        for j in range(10):\n",
        "            grad_W2[j][i] = sum(h[j] * dloss[i] / len(hidden_outputs) for h in hidden_outputs)\n",
        "        grad_b2[i] = dloss[i]\n",
        "\n",
        "    d_hidden = []\n",
        "    for h in hidden_outputs:\n",
        "        dh = [0] * 10\n",
        "        for i in range(10):\n",
        "            for j in range(vocab_size):\n",
        "                dh[i] += dloss[j] * W2[i][j]\n",
        "            dh[i] *= relu_derivative(h)[i]\n",
        "        d_hidden.append(dh)\n",
        "\n",
        "    grad_W1 = [[0] * 10 for _ in range(vocab_size)]\n",
        "    grad_b1 = [0] * 10\n",
        "\n",
        "    for i in range(10):\n",
        "        for j in range(vocab_size):\n",
        "            grad_W1[j][i] = sum(input_vectors[k][j] * d_hidden[k][i] for k in range(len(input_vectors)))\n",
        "        grad_b1[i] = sum(dh[i] for dh in d_hidden)\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        for j in range(10):\n",
        "            W1[i][j] -= learning_rate * grad_W1[i][j]\n",
        "\n",
        "    for i in range(10):\n",
        "        b1[i] -= learning_rate * grad_b1[i]\n",
        "\n",
        "    for i in range(10):\n",
        "        for j in range(vocab_size):\n",
        "            W2[i][j] -= learning_rate * grad_W2[i][j]\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        b2[i] -= learning_rate * grad_b2[i]\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss:.4f}\")\n",
        "\n",
        "hidden_outputs = []\n",
        "final_outputs = []\n",
        "for input_vec in input_vectors:\n",
        "    hidden = relu(add_bias(dot(input_vec, W1), b1))\n",
        "    output = add_bias(dot(hidden, W2), b2)\n",
        "    hidden_outputs.append(hidden)\n",
        "    final_outputs.append(output)\n",
        "\n",
        "output_mean = [sum(x)/len(x) for x in zip(*final_outputs)]\n",
        "predicted_idx = output_mean.index(max(output_mean))\n",
        "\n",
        "print(f\"\\nالكلمة المتوقعة: {idx_to_word[predicted_idx]}\")\n"
      ]
    }
  ]
}